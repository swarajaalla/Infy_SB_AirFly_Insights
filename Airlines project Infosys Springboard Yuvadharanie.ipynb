{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea830487-7e4b-4368-97ab-c052db21cc6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Step 1:Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac2e555b-0385-4db4-97e3-5cd218083610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Spark + Pandas\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"FlightDelayPreprocessing\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79baf9ba-f1b9-4fe0-8ba2-e878337edace",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Step 2: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1def9b4a-1435-4254-b85b-cab493066078",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# File path\n",
    "file_path = \"/Volumes/workspace/default/airlines/Flight_delay.csv\"\n",
    "\n",
    "# Load with Pandas (for quick exploration)\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Load with Spark (for large-scale processing)\n",
    "df_spark = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "print(\"✅ Dataset loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb7e1ffd-4b25-4356-8c68-a7533e904099",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Step 3: Explore Schema & Data Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "718a9481-5245-4764-925f-432ffa12742d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83D\uDCCC Pandas Info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 484551 entries, 0 to 484550\nData columns (total 29 columns):\n #   Column             Non-Null Count   Dtype \n---  ------             --------------   ----- \n 0   DayOfWeek          484551 non-null  int64 \n 1   Date               484551 non-null  object\n 2   DepTime            484551 non-null  int64 \n 3   ArrTime            484551 non-null  int64 \n 4   CRSArrTime         484551 non-null  int64 \n 5   UniqueCarrier      484551 non-null  object\n 6   Airline            484551 non-null  object\n 7   FlightNum          484551 non-null  int64 \n 8   TailNum            484551 non-null  object\n 9   ActualElapsedTime  484551 non-null  int64 \n 10  CRSElapsedTime     484551 non-null  int64 \n 11  AirTime            484551 non-null  int64 \n 12  ArrDelay           484551 non-null  int64 \n 13  DepDelay           484551 non-null  int64 \n 14  Origin             484551 non-null  object\n 15  Org_Airport        483374 non-null  object\n 16  Dest               484551 non-null  object\n 17  Dest_Airport       483072 non-null  object\n 18  Distance           484551 non-null  int64 \n 19  TaxiIn             484551 non-null  int64 \n 20  TaxiOut            484551 non-null  int64 \n 21  Cancelled          484551 non-null  int64 \n 22  CancellationCode   484551 non-null  object\n 23  Diverted           484551 non-null  int64 \n 24  CarrierDelay       484551 non-null  int64 \n 25  WeatherDelay       484551 non-null  int64 \n 26  NASDelay           484551 non-null  int64 \n 27  SecurityDelay      484551 non-null  int64 \n 28  LateAircraftDelay  484551 non-null  int64 \ndtypes: int64(20), object(9)\nmemory usage: 107.2+ MB\nNone\n\n\uD83D\uDCCC Null values (Pandas):\nDayOfWeek               0\nDate                    0\nDepTime                 0\nArrTime                 0\nCRSArrTime              0\nUniqueCarrier           0\nAirline                 0\nFlightNum               0\nTailNum                 0\nActualElapsedTime       0\nCRSElapsedTime          0\nAirTime                 0\nArrDelay                0\nDepDelay                0\nOrigin                  0\nOrg_Airport          1177\nDest                    0\nDest_Airport         1479\nDistance                0\nTaxiIn                  0\nTaxiOut                 0\nCancelled               0\nCancellationCode        0\nDiverted                0\nCarrierDelay            0\nWeatherDelay            0\nNASDelay                0\nSecurityDelay           0\nLateAircraftDelay       0\ndtype: int64\n\n\uD83D\uDCCC Spark Schema:\nroot\n |-- DayOfWeek: integer (nullable = true)\n |-- Date: date (nullable = true)\n |-- DepTime: integer (nullable = true)\n |-- ArrTime: integer (nullable = true)\n |-- CRSArrTime: integer (nullable = true)\n |-- UniqueCarrier: string (nullable = true)\n |-- Airline: string (nullable = true)\n |-- FlightNum: integer (nullable = true)\n |-- TailNum: string (nullable = true)\n |-- ActualElapsedTime: integer (nullable = true)\n |-- CRSElapsedTime: integer (nullable = true)\n |-- AirTime: integer (nullable = true)\n |-- ArrDelay: integer (nullable = true)\n |-- DepDelay: integer (nullable = true)\n |-- Origin: string (nullable = true)\n |-- Org_Airport: string (nullable = true)\n |-- Dest: string (nullable = true)\n |-- Dest_Airport: string (nullable = true)\n |-- Distance: integer (nullable = true)\n |-- TaxiIn: integer (nullable = true)\n |-- TaxiOut: integer (nullable = true)\n |-- Cancelled: integer (nullable = true)\n |-- CancellationCode: string (nullable = true)\n |-- Diverted: integer (nullable = true)\n |-- CarrierDelay: integer (nullable = true)\n |-- WeatherDelay: integer (nullable = true)\n |-- NASDelay: integer (nullable = true)\n |-- SecurityDelay: integer (nullable = true)\n |-- LateAircraftDelay: integer (nullable = true)\n\n\n\uD83D\uDCCC First 5 rows (Spark):\n+---------+----------+-------+-------+----------+-------------+--------------------+---------+-------+-----------------+--------------+-------+--------+--------+------+--------------------+----+--------------------+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n|DayOfWeek|      Date|DepTime|ArrTime|CRSArrTime|UniqueCarrier|             Airline|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|         Org_Airport|Dest|        Dest_Airport|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n+---------+----------+-------+-------+----------+-------------+--------------------+---------+-------+-----------------+--------------+-------+--------+--------+------+--------------------+----+--------------------+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n|        4|2019-01-03|   1829|   1959|      1925|           WN|Southwest Airline...|     3920| N464WN|               90|            90|     77|      34|      34|   IND|Indianapolis Inte...| BWI|Baltimore-Washing...|     515|     3|     10|        0|               N|       0|           2|           0|       0|            0|               32|\n|        4|2019-01-03|   1937|   2037|      1940|           WN|Southwest Airline...|      509| N763SW|              240|           250|    230|      57|      67|   IND|Indianapolis Inte...| LAS|McCarran Internat...|    1591|     3|      7|        0|               N|       0|          10|           0|       0|            0|               47|\n|        4|2019-01-03|   1644|   1845|      1725|           WN|Southwest Airline...|     1333| N334SW|              121|           135|    107|      80|      94|   IND|Indianapolis Inte...| MCO|Orlando Internati...|     828|     6|      8|        0|               N|       0|           8|           0|       0|            0|               72|\n|        4|2019-01-03|   1452|   1640|      1625|           WN|Southwest Airline...|      675| N286WN|              228|           240|    213|      15|      27|   IND|Indianapolis Inte...| PHX|Phoenix Sky Harbo...|    1489|     7|      8|        0|               N|       0|           3|           0|       0|            0|               12|\n|        4|2019-01-03|   1323|   1526|      1510|           WN|Southwest Airline...|        4| N674AA|              123|           135|    110|      16|      28|   IND|Indianapolis Inte...| TPA|Tampa Internation...|     838|     4|      9|        0|               N|       0|           0|           0|       0|            0|               16|\n+---------+----------+-------+-------+----------+-------------+--------------------+---------+-------+-----------------+--------------+-------+--------+--------+------+--------------------+----+--------------------+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\nonly showing top 5 rows\n\n\uD83D\uDCCC Null values (Spark):\n+---------+----+-------+-------+----------+-------------+-------+---------+-------+-----------------+--------------+-------+--------+--------+------+-----------+----+------------+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n|DayOfWeek|Date|DepTime|ArrTime|CRSArrTime|UniqueCarrier|Airline|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Org_Airport|Dest|Dest_Airport|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n+---------+----+-------+-------+----------+-------------+-------+---------+-------+-----------------+--------------+-------+--------+--------+------+-----------+----+------------+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n|        0|   0|      0|      0|         0|            0|      0|        0|      0|                0|             0|      0|       0|       0|     0|          0|   0|           0|       0|     0|      0|        0|               0|       0|           0|           0|       0|            0|                0|\n+---------+----+-------+-------+----------+-------------+-------+---------+-------+-----------------+--------------+-------+--------+--------+------+-----------+----+------------+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Pandas overview\n",
    "print(\"\\n\uD83D\uDCCC Pandas Info:\")\n",
    "print(df.info())\n",
    "print(\"\\n\uD83D\uDCCC Null values (Pandas):\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Spark overview\n",
    "print(\"\\n\uD83D\uDCCC Spark Schema:\")\n",
    "df_spark.printSchema()\n",
    "\n",
    "print(\"\\n\uD83D\uDCCC First 5 rows (Spark):\")\n",
    "df_spark.show(5)\n",
    "\n",
    "print(\"\\n\uD83D\uDCCC Null values (Spark):\")\n",
    "df_spark.select([\n",
    "    F.count(F.when(F.col(c).isNull(), c)).alias(c)\n",
    "    for c in df_spark.columns\n",
    "]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfcf7cf9-5ac6-40c2-8098-22c00d6f4105",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Step 4: Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61331e52-ff68-4e4f-babb-21be3e59d1a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Missing values handled\n"
     ]
    }
   ],
   "source": [
    "# Fill Org_Airport and Dest_Airport with \"Unknown\"\n",
    "df_spark = df_spark.fillna({\"Org_Airport\": \"Unknown\", \"Dest_Airport\": \"Unknown\"})\n",
    "\n",
    "# Handle CancellationCode → Replace empty values with \"Not Cancelled\"\n",
    "df_spark = df_spark.withColumn(\n",
    "    \"CancellationCode\",\n",
    "    F.when(F.col(\"CancellationCode\") == \"\", \"Not Cancelled\")\n",
    "     .otherwise(F.col(\"CancellationCode\"))\n",
    ")\n",
    "\n",
    "print(\"✅ Missing values handled\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50a53d22-cb8d-4d89-a1e4-507af3463b8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Step 5: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7915f82-88af-4b94-8666-38d558f64222",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Feature engineering completed\n"
     ]
    }
   ],
   "source": [
    "# Ensure Date is in date format\n",
    "df_spark = df_spark.withColumn(\"Date\", F.to_date(\"Date\", \"yyyy-MM-dd\"))\n",
    "\n",
    "# Extract Month\n",
    "df_spark = df_spark.withColumn(\"Month\", F.month(\"Date\"))\n",
    "\n",
    "# Extract Day of Week (1 = Sunday … 7 = Saturday)\n",
    "df_spark = df_spark.withColumn(\"DayNumber\", F.dayofweek(\"Date\"))\n",
    "\n",
    "# Extract Hour from DepTime (e.g. 1829 → 18)\n",
    "df_spark = df_spark.withColumn(\"Hour\", (F.col(\"DepTime\")/100).cast(\"int\"))\n",
    "\n",
    "# Create Route column (Origin-Dest)\n",
    "df_spark = df_spark.withColumn(\"Route\", F.concat_ws(\"-\", F.col(\"Origin\"), F.col(\"Dest\")))\n",
    "\n",
    "print(\"✅ Feature engineering completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dad644c-070f-4752-868e-f95cdd7ca41e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Step 6: Verify New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64f866c5-c72f-48a7-a69b-1056a73e873b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCCC Sample with new features:\n+----------+-----+---------+----+------+----+-------+\n|Date      |Month|DayNumber|Hour|Origin|Dest|Route  |\n+----------+-----+---------+----+------+----+-------+\n|2019-01-03|1    |5        |18  |IND   |BWI |IND-BWI|\n|2019-01-03|1    |5        |19  |IND   |LAS |IND-LAS|\n|2019-01-03|1    |5        |16  |IND   |MCO |IND-MCO|\n|2019-01-03|1    |5        |14  |IND   |PHX |IND-PHX|\n|2019-01-03|1    |5        |13  |IND   |TPA |IND-TPA|\n|2019-01-03|1    |5        |14  |ISP   |BWI |ISP-BWI|\n|2019-01-03|1    |5        |16  |ISP   |BWI |ISP-BWI|\n|2019-01-03|1    |5        |14  |ISP   |FLL |ISP-FLL|\n|2019-01-03|1    |5        |21  |ISP   |MCO |ISP-MCO|\n|2019-01-03|1    |5        |18  |ISP   |MDW |ISP-MDW|\n+----------+-----+---------+----+------+----+-------+\nonly showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"\uD83D\uDCCC Sample with new features:\")\n",
    "df_spark.select(\"Date\", \"Month\", \"DayNumber\", \"Hour\", \"Origin\", \"Dest\", \"Route\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac77f245-b306-4f73-a444-db0302166d6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Step 7: Save Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12ee0e63-8647-4542-8c84-59318df863bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:132)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:132)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:129)\n",
       "\tat scala.collection.immutable.Range.foreach(Range.scala:158)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:129)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:715)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:435)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:435)\n",
       "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:465)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:743)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:299)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:295)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:84)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:84)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:84)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:84)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:721)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$com$databricks$spark$chauffeur$Chauffeur$$nestedInanon$$receiveInternal$1.handleDriverRequest$1(Chauffeur.scala:918)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$com$databricks$spark$chauffeur$Chauffeur$$nestedInanon$$receiveInternal$1.$anonfun$applyOrElse$3(Chauffeur.scala:944)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:299)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:295)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$com$databricks$spark$chauffeur$Chauffeur$$nestedInanon$$receiveInternal$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:943)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$com$databricks$spark$chauffeur$Chauffeur$$nestedInanon$$receiveInternal$1.applyOrElse(Chauffeur.scala:998)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$com$databricks$spark$chauffeur$Chauffeur$$nestedInanon$$receiveInternal$1.applyOrElse(Chauffeur.scala:793)\n",
       "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:789)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:784)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:299)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:295)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:988)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:908)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:548)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:513)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$12(ActivityContextFactory.scala:914)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:299)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:295)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:58)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$2(ActivityContextFactory.scala:914)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:877)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:859)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$26(ActivityContextFactory.scala:331)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:299)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:295)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:58)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:331)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:513)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:408)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:111)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:299)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:295)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:46)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:111)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:46)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:93)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:132)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:132)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:129)",
        "\tat scala.collection.immutable.Range.foreach(Range.scala:158)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:129)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:715)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:435)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:435)",
        "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:465)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:743)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:299)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:295)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:84)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:84)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:84)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:84)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:721)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$com$databricks$spark$chauffeur$Chauffeur$$nestedInanon$$receiveInternal$1.handleDriverRequest$1(Chauffeur.scala:918)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$com$databricks$spark$chauffeur$Chauffeur$$nestedInanon$$receiveInternal$1.$anonfun$applyOrElse$3(Chauffeur.scala:944)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:299)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:295)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$com$databricks$spark$chauffeur$Chauffeur$$nestedInanon$$receiveInternal$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:943)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$com$databricks$spark$chauffeur$Chauffeur$$nestedInanon$$receiveInternal$1.applyOrElse(Chauffeur.scala:998)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$com$databricks$spark$chauffeur$Chauffeur$$nestedInanon$$receiveInternal$1.applyOrElse(Chauffeur.scala:793)",
        "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:789)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:784)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:299)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:295)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:988)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:908)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:548)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:513)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$12(ActivityContextFactory.scala:914)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:299)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:295)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:58)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$2(ActivityContextFactory.scala:914)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:877)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:859)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$26(ActivityContextFactory.scala:331)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:299)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:295)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:58)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:331)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:513)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:408)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:111)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:299)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:295)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:46)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:111)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:46)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:93)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.base/java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert Spark dataframe to Pandas\n",
    "df_pandas = df_spark.toPandas()\n",
    "\n",
    "# Save as a single CSV\n",
    "output_path = \"/Volumes/workspace/default/airlines/Flight_delay_cleaned.csv\"\n",
    "df_pandas.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✅ Cleaned CSV saved at: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99c2a721-2c63-4eef-bb3d-82295ef33842",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCCC Cleaned CSV Shape: (403795, 33)\n\n\uD83D\uDCCC First 5 rows:\n   DayOfWeek        Date  DepTime  ArrTime  ...  Month DayNumber Hour    Route\n0          4  2019-01-03     1829     1959  ...      1         5   18  IND-BWI\n1          4  2019-01-03     1937     2037  ...      1         5   19  IND-LAS\n2          4  2019-01-03     1644     1845  ...      1         5   16  IND-MCO\n3          4  2019-01-03     1452     1640  ...      1         5   14  IND-PHX\n4          4  2019-01-03     1323     1526  ...      1         5   13  IND-TPA\n\n[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_cleaned = pd.read_csv(\"/Volumes/workspace/default/airlines/Flight_delay_cleaned.csv\")\n",
    "print(\"\uD83D\uDCCC Cleaned CSV Shape:\", df_cleaned.shape)\n",
    "print(\"\\n\uD83D\uDCCC First 5 rows:\")\n",
    "print(df_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70c865f7-f333-49b9-8e9b-1c3d3fa4b315",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Step 8:Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ff0b0a1-af30-483a-88a2-01c6521369bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCCC FINAL SUMMARY (Pandas)\nRows: 403795, Columns: 33\n\n\uD83D\uDCCC Sample 5 rows:\n   DayOfWeek        Date  DepTime  ArrTime  ...  Month DayNumber Hour    Route\n0          4  2019-01-03     1829     1959  ...      1         5   18  IND-BWI\n1          4  2019-01-03     1937     2037  ...      1         5   19  IND-LAS\n2          4  2019-01-03     1644     1845  ...      1         5   16  IND-MCO\n3          4  2019-01-03     1452     1640  ...      1         5   14  IND-PHX\n4          4  2019-01-03     1323     1526  ...      1         5   13  IND-TPA\n\n[5 rows x 33 columns]\n\n\uD83D\uDCCC Null values per column:\nDayOfWeek               0\nDate                    0\nDepTime                 0\nArrTime                 0\nCRSArrTime              0\nUniqueCarrier           0\nAirline                 0\nFlightNum               0\nTailNum                 0\nActualElapsedTime       0\nCRSElapsedTime          0\nAirTime                 0\nArrDelay                0\nDepDelay                0\nOrigin                  0\nOrg_Airport           973\nDest                    0\nDest_Airport         1243\nDistance                0\nTaxiIn                  0\nTaxiOut                 0\nCancelled               0\nCancellationCode        0\nDiverted                0\nCarrierDelay            0\nWeatherDelay            0\nNASDelay                0\nSecurityDelay           0\nLateAircraftDelay       0\nMonth                   0\nDayNumber               0\nHour                    0\nRoute                   0\ndtype: int64\n\n\uD83D\uDCCC Summary statistics for numeric columns:\n           DayOfWeek        DepTime  ...      DayNumber           Hour\ncount  403795.000000  403795.000000  ...  403795.000000  403795.000000\nmean        3.978643    1557.317049  ...       4.005676      15.279778\nstd         1.955540     451.416440  ...       1.934270       4.519010\nmin         1.000000       1.000000  ...       1.000000       0.000000\n25%         2.000000    1229.000000  ...       2.000000      12.000000\n50%         4.000000    1614.000000  ...       4.000000      16.000000\n75%         5.000000    1924.000000  ...       6.000000      19.000000\nmax         7.000000    2400.000000  ...       7.000000      24.000000\n\n[8 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_cleaned = pd.read_csv(\"/Volumes/workspace/default/airlines/Flight_delay_cleaned.csv\")\n",
    "\n",
    "print(\"\uD83D\uDCCC FINAL SUMMARY (Pandas)\")\n",
    "\n",
    "# Rows & columns\n",
    "print(f\"Rows: {df_cleaned.shape[0]}, Columns: {df_cleaned.shape[1]}\")\n",
    "\n",
    "# Sample rows\n",
    "print(\"\\n\uD83D\uDCCC Sample 5 rows:\")\n",
    "print(df_cleaned.head())\n",
    "\n",
    "# Null values\n",
    "print(\"\\n\uD83D\uDCCC Null values per column:\")\n",
    "print(df_cleaned.isnull().sum())\n",
    "\n",
    "# Numeric statistics\n",
    "print(\"\\n\uD83D\uDCCC Summary statistics for numeric columns:\")\n",
    "print(df_cleaned.describe())\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Airlines project Infosys Springboard Yuvadharanie",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}